{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install \"numpy~=1.26.0\" \"ml-dtypes~=0.4.0\" \"tensorflow-intel==2.18.0\" \"mediapipe~=0.10.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization # For CNN\n",
    "# from tensorflow.keras.layers import LSTM, GRU # Optional: if you want to try other recurrent layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from constants import (\n",
    "    DATA_INPUT_PATH,\n",
    "    MODEL_PATH,        # This will be for the Keras model file\n",
    "    METADATA_PATH,     # This will be for the scaler and other metadata pickle file\n",
    "    PREDICTED_LANDMARK_NAMES, # Defines the target landmark columns\n",
    "    NUM_EMG_SENSORS,\n",
    "    WINDOW_SIZE,\n",
    "    WINDOW_STEP\n",
    "    # Add any other relevant constants like NUM_PREDICTED_LANDMARKS if defined,\n",
    "    # or derive NUM_PREDICTED_LANDMARKS from PREDICTED_LANDMARK_NAMES later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .csv(s) to dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Convert .csv(s) to dataframes and concatenate\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Constants Loading (Ensure these are available from Cell 3) ---\n",
    "# from constants import DATA_INPUT_PATH, NUM_EMG_SENSORS, PREDICTED_LANDMARK_NAMES\n",
    "# ---\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Set your input path from constants.py\n",
    "data_input_dir = Path(DATA_INPUT_PATH)\n",
    "\n",
    "# Find CSV files\n",
    "files = list(data_input_dir.glob(\"*.csv\"))\n",
    "print(f\"Looking for CSV files in: {data_input_dir.resolve()}\")\n",
    "print(f\"Found CSVs: {files}\")\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {data_input_dir.resolve()}. Please check DATA_INPUT_PATH in constants.py and your data directory.\")\n",
    "\n",
    "# Read all files\n",
    "for file_path in files:\n",
    "    try:\n",
    "        df_temp = pd.read_csv(file_path)\n",
    "        # Optional: Add a check for necessary columns *before* appending\n",
    "        # required_for_load = [f\"s{i}_norm\" for i in range(1, NUM_EMG_SENSORS + 1)] + [\"quat_w\"] + PREDICTED_LANDMARK_NAMES[:1]\n",
    "        # if not all(col in df_temp.columns for col in required_for_load):\n",
    "        #     print(f\"Warning: Skipping {file_path}, missing essential columns like sX_norm or landmarks.\")\n",
    "        #     continue\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"Successfully loaded {file_path}, shape: {df_temp.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        continue # Skip to next file if one is problematic\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No DataFrames were successfully loaded. Check CSV files for issues.\")\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "df_concat = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "print(f\"Shape after concatenation of all CSVs: {df_concat.shape}\")\n",
    "\n",
    "# --- Define Feature and Target Columns ---\n",
    "\n",
    "# <<< CHANGE: Use _norm suffix for EMG columns as these are the input features now >>>\n",
    "emg_cols = [f\"s{i}_norm\" for i in range(1, NUM_EMG_SENSORS + 1)] # e.g., ['s1_norm', ..., 's8_norm']\n",
    "imu_cols = [\"quat_w\", \"quatx\", \"quaty\", \"quatz\"]\n",
    "\n",
    "# Rename quaternion columns ONLY IF loading data that used old names (quat_x etc.)\n",
    "# Your newer data likely already has quatx, quaty, quatz based on collection.py\n",
    "cols_to_rename = {}\n",
    "if 'quat_x' in df_concat.columns and 'quatx' not in df_concat.columns: cols_to_rename['quat_x'] = 'quatx'\n",
    "if 'quat_y' in df_concat.columns and 'quaty' not in df_concat.columns: cols_to_rename['quat_y'] = 'quaty'\n",
    "if 'quat_z' in df_concat.columns and 'quatz' not in df_concat.columns: cols_to_rename['quat_z'] = 'quatz'\n",
    "if cols_to_rename:\n",
    "    print(f\"Found old quaternion column names. Renaming: {cols_to_rename}\")\n",
    "    df_concat.rename(columns=cols_to_rename, inplace=True)\n",
    "\n",
    "feature_columns = emg_cols + imu_cols # Feature columns now use pre-normalized EMG + quats\n",
    "target_landmark_columns = PREDICTED_LANDMARK_NAMES # Target columns remain the landmark coordinates\n",
    "\n",
    "# --- Validate Columns ---\n",
    "print(f\"\\nVerifying required columns...\")\n",
    "print(f\"Expected Feature Columns ({len(feature_columns)}): {feature_columns[:5]}...\")\n",
    "print(f\"Expected Target Columns ({len(target_landmark_columns)}): {target_landmark_columns[:5]}...\")\n",
    "\n",
    "available_columns_list = df_concat.columns.tolist()\n",
    "\n",
    "missing_features = [col for col in feature_columns if col not in available_columns_list]\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Critical Error: Missing required input feature columns in the loaded data: {missing_features}. \"\n",
    "                     f\"These columns (incl. sX_norm) should be generated by collection.py. Available columns: {available_columns_list}\")\n",
    "\n",
    "missing_targets = [col for col in target_landmark_columns if col not in available_columns_list]\n",
    "if missing_targets:\n",
    "    raise ValueError(f\"Critical Error: Missing required target landmark columns in the loaded data: {missing_targets}. \"\n",
    "                     f\"Check constants.PREDICTED_LANDMARK_NAMES vs CSV headers. Available columns: {available_columns_list}\")\n",
    "\n",
    "print(\"All required feature and target columns found.\")\n",
    "\n",
    "# --- Select Columns and Clean Data ---\n",
    "# Keep only the columns needed for windowing + potentially id/gesture_id for context\n",
    "columns_to_keep = []\n",
    "if 'timestamp' in available_columns_list: columns_to_keep.append('timestamp')\n",
    "if 'gesture_id' in available_columns_list: columns_to_keep.append('gesture_id')\n",
    "columns_to_keep.extend(feature_columns)\n",
    "columns_to_keep.extend(target_landmark_columns)\n",
    "final_columns_to_keep = sorted(list(set(columns_to_keep)), key=columns_to_keep.index) # Unique, ordered\n",
    "\n",
    "df_processed = df_concat[final_columns_to_keep].copy()\n",
    "print(f\"\\nShape after selecting relevant columns ({len(final_columns_to_keep)} columns): {df_processed.shape}\")\n",
    "\n",
    "# Drop rows with any NaNs in the essential feature/target columns\n",
    "# (Check only feature/target columns for NaNs that would break training)\n",
    "essential_cols = feature_columns + target_landmark_columns\n",
    "shape_before_na_drop = df_processed.shape\n",
    "df_processed.dropna(subset=essential_cols, inplace=True)\n",
    "print(f\"Shape after dropping rows with NaN in features/targets: {df_processed.shape}. (Dropped {shape_before_na_drop[0] - df_processed.shape[0]} rows)\")\n",
    "\n",
    "if df_processed.empty:\n",
    "    raise ValueError(\"DataFrame is empty after processing and NaN removal. Check raw data.\")\n",
    "\n",
    "# Drop duplicate rows (can be intensive, consider sampling if dataset is huge)\n",
    "shape_before_duplicates = df_processed.shape\n",
    "df_processed.drop_duplicates(inplace=True)\n",
    "print(f\"Shape after dropping duplicate rows: {df_processed.shape}. (Dropped {shape_before_duplicates[0] - df_processed.shape[0]} rows)\")\n",
    "\n",
    "if df_processed.empty:\n",
    "    raise ValueError(\"DataFrame is empty after dropping duplicates.\")\n",
    "\n",
    "print(f\"\\n--- Final Processed DataFrame Head (df_processed used for windowing) ---\")\n",
    "print(df_processed.head())\n",
    "print(f\"\\nUsing Feature columns ({len(feature_columns)}): {feature_columns}\")\n",
    "print(f\"Using Target landmark columns ({len(target_landmark_columns)}): {target_landmark_columns[:5]}... (first 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration_mean and calibration_std derived here are for an old Z-score normalization method. We are now using StandardScaler fitted on X_train. Kept for archival purposes.\n",
    "# --- OLD Calculate Calibration Stats from 'Rest' Data (Z-Score) ---\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from constants import NUM_EMG_SENSORS # Make sure NUM_EMG_SENSORS is defined in constants.py (should be 8)\n",
    "\n",
    "print(\"Calculating calibration statistics from 'Rest' data...\")\n",
    "\n",
    "# Select only the 'Rest' data (gesture_id == 0)\n",
    "rest_df = df[df['gesture_id'] == 0].copy()\n",
    "\n",
    "# Select only the EMG columns (s1 to s8)\n",
    "emg_columns = [f\"s{i}\" for i in range(1, NUM_EMG_SENSORS + 1)]\n",
    "\n",
    "# Select only IMU columns (quat)\n",
    "imu_columns = [\"quat_w\",\"quatx\",\"quaty\",\"quatz\"]\n",
    "\n",
    "# Get landmark data\n",
    "landmark_columns = [c for c in df.columns if c.endswith((\"_x\",\"_y\",\"_z\"))]\n",
    "print(f\"Landmark columns are: {landmark_columns}\")\n",
    "\n",
    "rest_emg_data = rest_df[emg_columns].values # Get as numpy array\n",
    "\n",
    "if len(rest_emg_data) > 0:\n",
    "    # Calculate mean and std dev for each EMG channel (column-wise)\n",
    "    calibration_mean = np.mean(rest_emg_data, axis=0)\n",
    "    calibration_std = np.std(rest_emg_data, axis=0)\n",
    "\n",
    "    # Avoid division by zero: set std dev to 1 if it's 0 or very close to 0\n",
    "    calibration_std[calibration_std < 1e-6] = 1.0\n",
    "\n",
    "    print(f\"Calculated Calibration Mean (shape {calibration_mean.shape}):\\n{np.round(calibration_mean, 2)}\")\n",
    "    print(f\"Calculated Calibration StdDev (shape {calibration_std.shape}):\\n{np.round(calibration_std, 2)}\")\n",
    "else:\n",
    "    print(\"ERROR: No 'Rest' data found to calculate calibration statistics!\")\n",
    "    # Handle this error appropriately - maybe exit or use default values\n",
    "    # Using default values for now, but this indicates a data problem\n",
    "    calibration_mean = np.zeros(NUM_EMG_SENSORS)\n",
    "    calibration_std = np.ones(NUM_EMG_SENSORS)\n",
    "    print(\"Using default calibration stats (mean=0, std=1). CHECK YOUR DATA.\")\n",
    "\n",
    "# Ensure these variables are available for the next cell\n",
    "# (They will be if run in the same kernel session)\n",
    "# --- END OF NEW CELL ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should directly follow Cell 5 (Data Loading/Processing)\n",
    "# It uses 'df_processed', 'feature_columns', and 'target_landmark_columns' from Cell 5.\n",
    "# It also uses WINDOW_SIZE and WINDOW_STEP from constants.py (imported in Cell 3).\n",
    "\n",
    "print(f\"\\n--- Starting Windowing Process ---\")\n",
    "print(f\"Using WINDOW_SIZE: {WINDOW_SIZE}, WINDOW_STEP: {WINDOW_STEP}\")\n",
    "print(f\"Input features for windowing ({len(feature_columns)}): {feature_columns}\")\n",
    "print(f\"Target features for windowing ({len(target_landmark_columns)}): {target_landmark_columns[:5]}... (first 5)\")\n",
    "\n",
    "X_sequences = []\n",
    "Y_targets = []\n",
    "\n",
    "# Ensure df_processed is not empty before windowing\n",
    "if df_processed.shape[0] < WINDOW_SIZE:\n",
    "    raise ValueError(\n",
    "        f\"Not enough data rows ({df_processed.shape[0]}) in df_processed to create even one window of size {WINDOW_SIZE}.\"\n",
    "        \" Check your data or filtering steps.\"\n",
    "    )\n",
    "\n",
    "# Create sequences\n",
    "# Loop from the first possible start index up to a point where a full window can still be formed.\n",
    "for start_index in range(0, df_processed.shape[0] - WINDOW_SIZE + 1, WINDOW_STEP):\n",
    "    end_index = start_index + WINDOW_SIZE\n",
    "    \n",
    "    # Extract input sequence (X): EMG + IMU data for the current window\n",
    "    # .values converts the DataFrame slice to a NumPy array\n",
    "    input_window_data = df_processed[feature_columns].iloc[start_index:end_index].values\n",
    "    X_sequences.append(input_window_data)\n",
    "    \n",
    "    # Extract target (Y): Landmark coordinates at the END of the current window\n",
    "    # .values converts the Series to a NumPy array\n",
    "    target_values_at_window_end = df_processed[target_landmark_columns].iloc[end_index - 1].values\n",
    "    Y_targets.append(target_values_at_window_end)\n",
    "\n",
    "if not X_sequences:\n",
    "    raise ValueError(\"No sequences were generated. This might happen if df_processed has fewer rows than WINDOW_SIZE.\")\n",
    "\n",
    "# Convert lists of sequences/targets to NumPy arrays\n",
    "X_win = np.array(X_sequences)\n",
    "Y = np.array(Y_targets) # Renaming Y_targets to Y for consistency with subsequent cells\n",
    "\n",
    "print(f\"\\n--- Windowing Complete ---\")\n",
    "print(f\"Shape of input sequences (X_win): {X_win.shape}\")  # Expected: (num_samples, WINDOW_SIZE, num_input_features)\n",
    "print(f\"Shape of target landmarks (Y): {Y.shape}\")      # Expected: (num_samples, num_target_landmark_coords)\n",
    "\n",
    "# Basic validation of shapes\n",
    "# num_input_features should be NUM_EMG_SENSORS (8) + number of IMU channels (4) = 12\n",
    "expected_num_input_features = NUM_EMG_SENSORS + len(imu_cols) \n",
    "if X_win.shape[2] != expected_num_input_features:\n",
    "    print(f\"Warning: X_win.shape[2] (number of features) is {X_win.shape[2]}, but expected {expected_num_input_features}.\")\n",
    "\n",
    "# num_target_values should be the total number of landmark coordinates defined in PREDICTED_LANDMARK_NAMES\n",
    "expected_num_target_values = len(PREDICTED_LANDMARK_NAMES)\n",
    "if Y.shape[1] != expected_num_target_values:\n",
    "    print(f\"Warning: Y.shape[1] (number of target values) is {Y.shape[1]}, but expected {expected_num_target_values} based on PREDICTED_LANDMARK_NAMES.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data, Scale Input Features not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Split Data (No Scaling Needed)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# NO StandardScaler import needed here\n",
    "\n",
    "# Make sure X_win and Y are defined from Cell 7 (Windowing)\n",
    "if 'X_win' not in locals() or 'Y' not in locals():\n",
    "    raise NameError(\"X_win or Y is not defined. Ensure Cell 7 (Windowing) was run successfully.\")\n",
    "\n",
    "print(f\"\\n--- Starting Data Split ---\")\n",
    "print(f\"Original X_win shape: {X_win.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "# 1) Split into Training + Validation vs. Test sets (e.g., 80% train+val, 20% test)\n",
    "X_trainval, X_test, Y_trainval, Y_test = train_test_split(\n",
    "    X_win, Y, test_size=0.20, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "# 2) Split Training + Validation into actual Training vs. Validation sets (e.g., 80% * 0.75 = 60% train, 80% * 0.25 = 20% val)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_trainval, Y_trainval, test_size=0.25, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(\"\\nShapes after splitting:\")\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_val shape:   {X_val.shape}, Y_val shape:   {Y_val.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}, Y_test shape:  {Y_test.shape}\")\n",
    "\n",
    "# <<< SCALING BLOCK REMOVED >>>\n",
    "# Input data (X_train, X_val, X_test) now uses pre-normalized EMG columns (sX_norm)\n",
    "# and raw quaternion data. We will feed this directly to the model.\n",
    "# Target data (Y_train, Y_val, Y_test) contains normalized landmark coordinates.\n",
    "\n",
    "# Define constants needed for model input shape based on the data\n",
    "num_train_samples, window_size_const, num_input_features_const = X_train.shape\n",
    "print(f\"\\nData shapes confirmed for model: window_size={window_size_const}, num_input_features={num_input_features_const}\")\n",
    "\n",
    "print(f\"--- Data Split Complete (No Scaling Applied in this Notebook) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, compile & train landmark regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Build, compile & train landmark regression model\n",
    "\n",
    "# Imports should be at the top (Cell 3)\n",
    "# Ensure necessary variables from Cell 9 are available:\n",
    "# X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "# window_size_const, num_input_features_const\n",
    "if 'X_train' not in locals(): raise NameError(\"Training data (X_train) not found. Ensure Cell 9 ran.\")\n",
    "if 'window_size_const' not in locals(): raise NameError(\"window_size_const not found. Ensure Cell 9 ran.\")\n",
    "if 'num_input_features_const' not in locals(): raise NameError(\"num_input_features_const not found. Ensure Cell 9 ran.\")\n",
    "\n",
    "print(f\"\\n--- Building and Training Model ---\")\n",
    "\n",
    "n_outputs = Y_train.shape[1] # Number of target landmark coordinates\n",
    "print(f\"Model output layer size (n_outputs): {n_outputs}\")\n",
    "print(f\"Model input shape: (window_size={window_size_const}, features={num_input_features_const})\")\n",
    "\n",
    "# Define the Keras Sequential model (architecture remains the same)\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(window_size_const, num_input_features_const), name=\"conv1d_1\"),\n",
    "    MaxPooling1D(pool_size=2, name=\"maxpool1d_1\"),\n",
    "    BatchNormalization(name=\"batchnorm_1\"),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', name=\"conv1d_2\"),\n",
    "    MaxPooling1D(pool_size=2, name=\"maxpool1d_2\"),\n",
    "    BatchNormalization(name=\"batchnorm_2\"),\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(units=128, activation='relu', name=\"dense_1\"),\n",
    "    Dropout(rate=0.3, name=\"dropout_1\"),\n",
    "    Dense(units=64, activation='relu', name=\"dense_2\"),\n",
    "    Dense(units=n_outputs, activation=\"linear\", name=\"output_landmarks\") # Linear activation for regression\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "model_path_obj = Path(MODEL_PATH) # Should point to .keras file now\n",
    "model_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Models checkpoints will be saved to: {model_path_obj.resolve()}\")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_PATH, save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"\\nStarting model training (using pre-normalized EMG input)...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train,                       # <<< USE UNSCALED X_train >>>\n",
    "    validation_data=(X_val, Y_val),       # <<< USE UNSCALED X_val >>>\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel training finished.\")\n",
    "\n",
    "# Load the best model saved by ModelCheckpoint\n",
    "print(f\"Loading the best model saved at: {MODEL_PATH}\")\n",
    "if not model_path_obj.exists():\n",
    "    print(f\"Warning: Best model file {MODEL_PATH} not found. Using model from last epoch.\")\n",
    "    # Ensure fallback model is compiled if needed\n",
    "    if not getattr(model, '_is_compiled', True):\n",
    "        print(\"Compiling model from last epoch...\")\n",
    "        model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "else:\n",
    "    try:\n",
    "        # Load model (using compile=False as robust workaround for potential loading issues)\n",
    "        model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "        print(\"Best model structure/weights loaded successfully (compile=False).\")\n",
    "        # Re-compile the loaded model\n",
    "        model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "        print(\"Best model re-compiled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading/compiling best model: {e}. Using model from last epoch.\")\n",
    "        # Compile fallback model if necessary\n",
    "        if not getattr(model, '_is_compiled', True):\n",
    "             print(\"Compiling model from last epoch (fallback)...\")\n",
    "             model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Evaluate the loaded model on the UNscaled test set\n",
    "print(\"\\nEvaluating final model on the test set...\")\n",
    "if model is not None:\n",
    "    test_loss, test_mae = model.evaluate(X_test, Y_test, verbose=0) # <<< USE UNSCALED X_test >>>\n",
    "    print(f\"Test Set Performance: Loss (MSE) = {test_loss:.4f}, Metric (MAE) = {test_mae:.4f}\")\n",
    "else:\n",
    "    print(\"ERROR: Model object is None, cannot evaluate.\")\n",
    "print(f\"--- Model Building and Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model and the scalar + feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Save Metadata Artifacts (No Scaler)\n",
    "\n",
    "print(f\"\\n--- Saving Metadata Artifacts (No Scaler) ---\")\n",
    "\n",
    "metadata_path_obj = Path(METADATA_PATH)\n",
    "metadata_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure required variables are available\n",
    "if 'PREDICTED_LANDMARK_NAMES' not in locals(): raise NameError(\"PREDICTED_LANDMARK_NAMES not found.\")\n",
    "if 'feature_columns' not in locals(): raise NameError(\"feature_columns (using sX_norm) not found.\") # Should use sX_norm now\n",
    "\n",
    "# <<< CHANGE: Removed 'input_scaler' from metadata_to_save >>>\n",
    "metadata_to_save = {\n",
    "    \"predicted_landmark_names_ordered\": PREDICTED_LANDMARK_NAMES,\n",
    "    \"input_feature_names_ordered\": feature_columns # Contains sX_norm + quat names\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(METADATA_PATH, \"wb\") as f:\n",
    "        pickle.dump(metadata_to_save, f)\n",
    "    print(f\"Successfully saved metadata to: {metadata_path_obj.resolve()}\")\n",
    "    print(f\"Metadata now contains: {list(metadata_to_save.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving metadata to {METADATA_PATH}: {e}\")\n",
    "\n",
    "print(f\"--- Metadata Saving Complete ---\")\n",
    "\n",
    "# Reminder: Keras model (.keras) was saved in Cell 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from constants import DATA_INPUT_PATH, MODEL_INPUT_PATH, MODEL_PATH, METADATA_PATH\n",
    "\n",
    "print(\"DATA_INPUT_PATH ->\", DATA_INPUT_PATH)\n",
    "print(\"  contains:\", os.listdir(DATA_INPUT_PATH))\n",
    "print()\n",
    "print(\"MODEL_INPUT_PATH ->\", MODEL_INPUT_PATH)\n",
    "print(\"  contains:\", os.listdir(MODEL_INPUT_PATH))\n",
    "print()\n",
    "print(\"MODEL_PATH       ->\", MODEL_PATH, \"exists?\", os.path.exists(MODEL_PATH))\n",
    "print(\"METADATA_PATH    ->\", METADATA_PATH, \"exists?\", os.path.exists(METADATA_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "print(constants.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO: test the new model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: DEMO: test the new model for accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt # Should already be imported\n",
    "\n",
    "print(f\"\\n--- Evaluating Trained Regression Model with Loaded Artifacts ---\")\n",
    "\n",
    "# 1. Load the trained regression Keras model\n",
    "model_path_obj = Path(MODEL_PATH) # Should point to .keras file\n",
    "if not model_path_obj.exists(): raise FileNotFoundError(...)\n",
    "print(f\"Loading regression model from: {model_path_obj.resolve()}\")\n",
    "try:\n",
    "    # Load using compile=False for robustness\n",
    "    loaded_regression_model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "    print(\"Regression model structure/weights loaded successfully (compile=False).\")\n",
    "    # Re-compile\n",
    "    loaded_regression_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "    print(\"Model re-compiled successfully.\")\n",
    "    loaded_regression_model.summary()\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Error loading/compiling Keras model from {MODEL_PATH}: {e}\")\n",
    "\n",
    "\n",
    "# 2. Load the metadata (landmark names, feature names - NO SCALER)\n",
    "metadata_path_obj = Path(METADATA_PATH)\n",
    "if not metadata_path_obj.exists(): raise FileNotFoundError(...)\n",
    "print(f\"Loading metadata from: {metadata_path_obj.resolve()}\")\n",
    "try:\n",
    "    with open(METADATA_PATH, \"rb\") as f: loaded_metadata = pickle.load(f)\n",
    "\n",
    "    # <<< CHANGE: Do not load/expect 'input_scaler' >>>\n",
    "    loaded_pred_landmark_names = loaded_metadata.get(\"predicted_landmark_names_ordered\")\n",
    "    loaded_input_feature_names = loaded_metadata.get(\"input_feature_names_ordered\") # Should contain sX_norm\n",
    "\n",
    "    # <<< CHANGE: Check required keys excluding scaler >>>\n",
    "    if loaded_pred_landmark_names is None or loaded_input_feature_names is None:\n",
    "        raise ValueError(f\"Metadata file {METADATA_PATH} is missing required keys. Found: {list(loaded_metadata.keys())}\")\n",
    "\n",
    "    print(\"Metadata loaded successfully.\")\n",
    "    print(f\"  Number of predicted landmark names: {len(loaded_pred_landmark_names)}\")\n",
    "    print(f\"  Number of input feature names: {len(loaded_input_feature_names)}\")\n",
    "    print(f\"  Input feature names example: {loaded_input_feature_names[:5]}...\") # Verify it shows _norm\n",
    "\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Error loading or parsing metadata from {METADATA_PATH}: {e}\")\n",
    "\n",
    "\n",
    "# 3. Use some data from X_test and Y_test (defined in Cell 9) for demonstration\n",
    "if 'X_test' not in locals() or 'Y_test' not in locals(): raise NameError(...) # X_test contains sX_norm\n",
    "\n",
    "num_samples_to_demo = min(5, X_test.shape[0])\n",
    "if num_samples_to_demo == 0:\n",
    "    print(\"No samples in X_test to demonstrate.\")\n",
    "else:\n",
    "    # <<< CHANGE: Use X_test directly as it contains pre-normalized EMG >>>\n",
    "    sample_X_for_prediction = X_test[:num_samples_to_demo]\n",
    "    sample_Y_true = Y_test[:num_samples_to_demo]\n",
    "    print(f\"\\nUsing first {num_samples_to_demo} samples from X_test (which contains sX_norm) for prediction...\")\n",
    "    print(f\"Shape of sample_X_for_prediction: {sample_X_for_prediction.shape}\")\n",
    "\n",
    "    # <<< REMOVED: Step 4 (applying scaler) is no longer needed >>>\n",
    "\n",
    "    # 5. Make predictions with the loaded model\n",
    "    predicted_Y_values = loaded_regression_model.predict(sample_X_for_prediction)\n",
    "    print(f\"Shape of predicted_Y_values: {predicted_Y_values.shape}\")\n",
    "\n",
    "\n",
    "    # 6. Visualize/Compare some predictions against true values\n",
    "    num_coords_to_plot_per_sample = min(3, len(loaded_pred_landmark_names))\n",
    "\n",
    "    for i in range(num_samples_to_demo):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        plt.figure(figsize=(12, num_coords_to_plot_per_sample * 2))\n",
    "        plt.suptitle(f\"Sample {i+1}: Predicted vs. True Landmark Coordinates\", fontsize=14)\n",
    "\n",
    "        for coord_idx in range(num_coords_to_plot_per_sample):\n",
    "            true_val = sample_Y_true[i, coord_idx]\n",
    "            pred_val = predicted_Y_values[i, coord_idx]\n",
    "            landmark_name = loaded_pred_landmark_names[coord_idx]\n",
    "\n",
    "            print(f\"  Landmark: {landmark_name}, True: {true_val:.4f}, Predicted: {pred_val:.4f}, Diff: {abs(true_val - pred_val):.4f}\")\n",
    "\n",
    "            ax = plt.subplot(num_coords_to_plot_per_sample, 1, coord_idx + 1)\n",
    "            ax.bar(['True Value', 'Predicted Value'], [true_val, pred_val], color=['skyblue', 'salmon'])\n",
    "            ax.set_title(f\"{landmark_name}\")\n",
    "            ax.set_ylabel(\"Coordinate Value\")\n",
    "            ax.grid(axis='y', linestyle='--')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    # Optional: Re-evaluate on the full X_test (unscaled) if desired\n",
    "    if 'X_test' in locals() and 'Y_test' in locals():\n",
    "         print(\"\\nRe-evaluating the loaded model on the full test set (using pre-normalized EMG)...\")\n",
    "         # Ensure model is compiled before evaluate\n",
    "         if not getattr(loaded_regression_model, '_is_compiled', True):\n",
    "             loaded_regression_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "         full_test_loss, full_test_mae = loaded_regression_model.evaluate(X_test, Y_test, verbose=0) # Use X_test\n",
    "         print(f\"  Full Test Set Performance: Loss (MSE) = {full_test_loss:.4f}, Metric (MAE) = {full_test_mae:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Evaluation with Loaded Artifacts Complete ---\")\n",
    "\n",
    "# Define num_input_features_const if not defined (needed for old Cell 17 reshape - now removed)\n",
    "if 'num_input_features_const' not in locals() and 'loaded_input_feature_names' in locals():\n",
    "     num_input_features_const = len(loaded_input_feature_names)\n",
    "     print(f\"Note: 'num_input_features_const' was derived from loaded metadata as {num_input_features_const}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
