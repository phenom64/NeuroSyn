{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydot\n",
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from constants import (\n",
    "    DATA_INPUT_PATH,\n",
    "    MODEL_PATH,\n",
    "    METADATA_PATH,\n",
    ")\n",
    "\n",
    "from constants import CLASSES\n",
    "num_classes = len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the files in the data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all of the files in the data folder\n",
    "files_in_folder = Path(DATA_INPUT_PATH).glob(\"*.csv\")\n",
    "\n",
    "files = [x for x in files_in_folder]\n",
    "print([file for file in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .csv(s) to dataframes and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the files\n",
    "dfs = []\n",
    "print(\"Looking in:\", DATA_INPUT_PATH)\n",
    "print(\"Found CSVs:\", list(Path(DATA_INPUT_PATH).glob(\"*.csv\")))\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(str(file))\n",
    "    dfs.append(df)\n",
    "        \n",
    "# Convert the data to a DataFrame\n",
    "df = pd.concat([x for x in dfs], axis=0)\n",
    "\n",
    "# after concatenating dfs…\n",
    "columns = [\"gesture_id\"] \\\n",
    "        + [f\"s{i}\" for i in range(1,9)] \\\n",
    "        + [\"quat_w\",\"quat_x\",\"quat_y\",\"quat_z\"]\n",
    "df = df[columns]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Before removing duplicates\n",
    "print(f\"Shape of dataframe before removing duplicates {df.shape}\")\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Shape of dataframe after removing duplicates {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW CELL: Calculate Calibration Stats from 'Rest' Data ---\n",
    "import numpy as np\n",
    "from constants import NUM_EMG_SENSORS # Make sure NUM_EMG_SENSORS is defined in constants.py (should be 8)\n",
    "\n",
    "print(\"Calculating calibration statistics from 'Rest' data...\")\n",
    "\n",
    "# Select only the 'Rest' data (gesture_id == 0)\n",
    "rest_df = df[df['gesture_id'] == 0].copy()\n",
    "\n",
    "# Select only the EMG columns (s1 to s8)\n",
    "emg_columns = [f\"s{i}\" for i in range(1, NUM_EMG_SENSORS + 1)]\n",
    "rest_emg_data = rest_df[emg_columns].values # Get as numpy array\n",
    "\n",
    "if len(rest_emg_data) > 0:\n",
    "    # Calculate mean and std dev for each EMG channel (column-wise)\n",
    "    calibration_mean = np.mean(rest_emg_data, axis=0)\n",
    "    calibration_std = np.std(rest_emg_data, axis=0)\n",
    "\n",
    "    # Avoid division by zero: set std dev to 1 if it's 0 or very close to 0\n",
    "    calibration_std[calibration_std < 1e-6] = 1.0\n",
    "\n",
    "    print(f\"Calculated Calibration Mean (shape {calibration_mean.shape}):\\n{np.round(calibration_mean, 2)}\")\n",
    "    print(f\"Calculated Calibration StdDev (shape {calibration_std.shape}):\\n{np.round(calibration_std, 2)}\")\n",
    "else:\n",
    "    print(\"ERROR: No 'Rest' data found to calculate calibration statistics!\")\n",
    "    # Handle this error appropriately - maybe exit or use default values\n",
    "    # Using default values for now, but this indicates a data problem\n",
    "    calibration_mean = np.zeros(NUM_EMG_SENSORS)\n",
    "    calibration_std = np.ones(NUM_EMG_SENSORS)\n",
    "    print(\"Using default calibration stats (mean=0, std=1). CHECK YOUR DATA.\")\n",
    "\n",
    "# Ensure these variables are available for the next cell\n",
    "# (They will be if run in the same kernel session)\n",
    "# --- END OF NEW CELL ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFIED CELL 5: Sliding-window Feature Extraction with Z-Score ---\n",
    "import numpy as np\n",
    "from constants import WINDOW_SIZE, WINDOW_STEP, NUM_EMG_SENSORS # Added NUM_EMG_SENSORS\n",
    "\n",
    "print(\"Starting feature extraction with Z-score normalization...\")\n",
    "\n",
    "# build channel list in correct order (ensure this matches df columns)\n",
    "channels = [f\"s{i}\" for i in range(1, NUM_EMG_SENSORS + 1)] + [\"quat_w\", \"quat_x\", \"quat_y\", \"quat_z\"]\n",
    "raw = df[channels].values          # shape = [total_samples, 12]\n",
    "labels = df[\"gesture_id\"].values   # shape = [total_samples,]\n",
    "\n",
    "X, y = [], []\n",
    "num_windows = 0 # Counter for debugging\n",
    "\n",
    "for start in range(0, len(raw) - WINDOW_SIZE + 1, WINDOW_STEP):\n",
    "    num_windows += 1\n",
    "    window = raw[start : start + WINDOW_SIZE]     # shape = [WINDOW_SIZE, 12]\n",
    "\n",
    "    # Separate EMG and IMU parts of the window\n",
    "    emg_part = window[:, :NUM_EMG_SENSORS]     # shape = [WINDOW_SIZE, 8]\n",
    "    imu_part = window[:, NUM_EMG_SENSORS:]     # shape = [WINDOW_SIZE, 4]\n",
    "\n",
    "    # <<< Apply Z-score normalization to EMG part using calculated stats >>>\n",
    "    # Ensure calibration_mean and calibration_std were calculated in the previous cell\n",
    "    try:\n",
    "        normalized_emg_part = (emg_part - calibration_mean) / calibration_std\n",
    "    except NameError:\n",
    "         print(\"ERROR: calibration_mean or calibration_std not defined. Make sure the previous cell was run.\")\n",
    "         # Handle error - maybe break or sys.exit()\n",
    "         break # Stop processing if calibration stats are missing\n",
    "\n",
    "\n",
    "    # <<< Calculate RMS on NORMALIZED EMG and ORIGINAL IMU >>>\n",
    "    rms_emg = np.sqrt(np.mean(normalized_emg_part**2, axis=0))  # Use normalized EMG\n",
    "    rms_imu = np.sqrt(np.mean(imu_part**2, axis=0))           # Use original IMU\n",
    "\n",
    "    # Concatenate features\n",
    "    rms_feat = np.concatenate([rms_emg, rms_imu])\n",
    "    X.append(rms_feat)\n",
    "\n",
    "    # Assign the window’s “center” label\n",
    "    center_idx = start + WINDOW_SIZE // 2\n",
    "    y.append(labels[center_idx])\n",
    "\n",
    "X = np.array(X)   # shape = [n_windows, 12]\n",
    "y = np.array(y)   # shape = [n_windows,]\n",
    "\n",
    "print(f\"Processed {num_windows} windows.\")\n",
    "print(\"X shape (RMS features):\", X.shape)\n",
    "print(\"y shape (labels):\", y.shape)\n",
    "# --- END OF MODIFIED CELL 5 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, split, and one-shot encode RMS feature windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ---- scale features ----\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---- one-hot labels ----\n",
    "y_cat = to_categorical(y, num_classes)\n",
    "\n",
    "# ---- train / test split ----\n",
    "# stratify=y ensures each class is proportionally represented\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_cat,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"  y_train:\", y_train.shape)\n",
    "print(\"X_test: \", X_test.shape, \"  y_test: \",  y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, compile & train a simple dense classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model and the scalar + feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFIED CELL 8: Save Model and Metadata ---\n",
    "# make sure the folder exists\n",
    "(Path(MODEL_PATH).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the trained model (this line is unchanged)\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Saved model to\", MODEL_PATH)\n",
    "\n",
    "# pickle the scaler, feature names, AND calibration stats\n",
    "with open(METADATA_PATH, \"wb\") as f:\n",
    "    # channels list we used for X is:\n",
    "    feature_names = [f\"s{i}\" for i in range(1, NUM_EMG_SENSORS + 1)] + [\"quat_w\", \"quat_x\", \"quat_y\", \"quat_z\"]\n",
    "    # <<< Save all three items in a tuple >>>\n",
    "    try:\n",
    "        pickle.dump((scaler, feature_names, calibration_mean, calibration_std), f)\n",
    "        print(\"Saved metadata (scaler, features, calib_mean, calib_std) to\", METADATA_PATH)\n",
    "    except NameError:\n",
    "        print(\"ERROR: Could not save metadata. Was calibration_mean/std calculated earlier?\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving metadata: {e}\")\n",
    "\n",
    "\n",
    "# --- END OF MODIFIED CELL 8 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from constants import DATA_INPUT_PATH, MODEL_INPUT_PATH, MODEL_PATH, METADATA_PATH\n",
    "\n",
    "print(\"DATA_INPUT_PATH ->\", DATA_INPUT_PATH)\n",
    "print(\"  contains:\", os.listdir(DATA_INPUT_PATH))\n",
    "print()\n",
    "print(\"MODEL_INPUT_PATH ->\", MODEL_INPUT_PATH)\n",
    "print(\"  contains:\", os.listdir(MODEL_INPUT_PATH))\n",
    "print()\n",
    "print(\"MODEL_PATH       ->\", MODEL_PATH, \"exists?\", os.path.exists(MODEL_PATH))\n",
    "print(\"METADATA_PATH    ->\", METADATA_PATH, \"exists?\", os.path.exists(METADATA_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "print(constants.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo results:\n",
      " • True:  0 Rest              → Predicted:  0 Rest\n",
      " • True:  1 Wrist Flexion     → Predicted:  1 Wrist Flexion\n",
      " • True:  2 Wrist Extension   → Predicted:  2 Wrist Extension\n",
      " • True:  3 Elbow Flexion     → Predicted:  2 Wrist Extension\n",
      " • True:  4 Elbow Extension   → Predicted:  4 Elbow Extension\n",
      " • True:  5 Hand Close        → Predicted:  5 Hand Close\n",
      " • True:  6 Hand Open         → Predicted:  6 Hand Open\n",
      " • True:  7 Forearm Pronation → Predicted:  7 Forearm Pronation\n",
      " • True:  8 Forearm Supination → Predicted:  8 Forearm Supination\n",
      "\n",
      "Demo accuracy (RMS‐window per class): 89%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "from constants import DATA_INPUT_PATH, MODEL_PATH, METADATA_PATH, CLASSES, WINDOW_SIZE\n",
    "\n",
    "# — 1) load the freshest CSV —\n",
    "data_dir = Path(DATA_INPUT_PATH)\n",
    "csv_path = max(data_dir.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime)\n",
    "df      = pd.read_csv(csv_path)\n",
    "\n",
    "# — 2) load scaler + feature names —\n",
    "with open(METADATA_PATH, \"rb\") as f:\n",
    "    scaler, feature_names, calibration_mean, calibration_std = pickle.load(f)\n",
    "\n",
    "# — 3) load your trained model —\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# — 4) for each class, grab the first 100 consecutive rows of that gesture,\n",
    "#         compute the 12-dim RMS feature vector, and predict —\n",
    "demo_rows = []\n",
    "for gid, name in CLASSES.items():\n",
    "    sub = df[df[\"gesture_id\"] == gid]\n",
    "    if len(sub) < WINDOW_SIZE:\n",
    "        print(f\"⚠️  not enough samples for '{name}' (need {WINDOW_SIZE}, got {len(sub)})\")\n",
    "        continue\n",
    "    \n",
    "    window  = sub[feature_names].values[:WINDOW_SIZE]\n",
    "    # Separate EMG/IMU parts from the 'window' variable\n",
    "    emg_part = window[:, :NUM_EMG_SENSORS]\n",
    "    imu_part = window[:, NUM_EMG_SENSORS:]\n",
    "\n",
    "    # Apply Z-score normalization to EMG part using LOADED stats\n",
    "    # (Make sure calibration_mean and calibration_std were loaded correctly earlier in this cell)\n",
    "    try:\n",
    "        normalized_emg_part = (emg_part - calibration_mean) / calibration_std\n",
    "    except NameError:\n",
    "        print(f\"ERROR: calibration_mean/std not available for demo processing (Class {gid}). Skipping.\")\n",
    "        continue # Skip to the next class if stats are missing\n",
    "\n",
    "    # Calculate RMS on normalized EMG and raw IMU\n",
    "    rms_emg = np.sqrt(np.mean(normalized_emg_part**2, axis=0))\n",
    "    rms_imu = np.sqrt(np.mean(imu_part**2, axis=0))\n",
    "\n",
    "    # Concatenate to get the final feature vector for the demo window\n",
    "    rms_vec = np.concatenate([rms_emg, rms_imu])\n",
    "\n",
    "    demo_rows.append((gid, name, rms_vec))\n",
    "\n",
    "# assemble our demo set\n",
    "y_true = [gid for gid,_,_ in demo_rows]\n",
    "X_demo = np.vstack([vec for *_, vec in demo_rows])\n",
    "\n",
    "# — 5) scale & predict —\n",
    "X_scaled = scaler.transform(X_demo)\n",
    "preds     = model.predict(X_scaled, verbose=0)\n",
    "pred_ids  = preds.argmax(axis=1)\n",
    "\n",
    "# — 6) report —\n",
    "print(\"Demo results:\")\n",
    "for (true_id, true_name, _), pred in zip(demo_rows, pred_ids):\n",
    "    print(f\" • True: {true_id:2d} {true_name:17s} → Predicted: {pred:2d} {CLASSES[pred]}\")\n",
    "    \n",
    "acc = np.mean(np.array(pred_ids) == np.array(y_true))\n",
    "print(f\"\\nDemo accuracy (RMS‐window per class): {acc:.0%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
